{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "cloud-ml-housing-prices.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruqyai/Deploy-a-Model-on-Cloud-ML-Engine/blob/master/cloud_ml_housing_prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELlMMfKXS8gf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Predicting Housing Prices using Tensorflow + Cloud ML Engine\n",
        "\n",
        "This notebook will show you how to create a tensorflow model, train it on the cloud in a distributed fashion across multiple CPUs or GPUs, explore the results using Tensorboard, and finally deploy the model for online prediction. We will demonstrate this by building a model to predict housing prices.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56zoYL0jS8gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Ai60S7S8gm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3EcoeReS8gq",
        "colab_type": "text"
      },
      "source": [
        "## Steps\n",
        "\n",
        "1. Load raw data\n",
        "\n",
        "2. Write Tensorflow Code\n",
        "\n",
        " 1. Define Feature Columns\n",
        " \n",
        " 2. Define Estimator\n",
        "\n",
        " 3. Define Input Function\n",
        " \n",
        " 4. Define Serving Function\n",
        "\n",
        " 5. Define Train and Eval Function\n",
        "\n",
        "3. Package Code\n",
        "\n",
        "4. Train\n",
        "\n",
        "5. Inspect Results\n",
        "\n",
        "6. Deploy Model\n",
        "\n",
        "7. Get Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFZy93d6S8gq",
        "colab_type": "text"
      },
      "source": [
        "### 1) Load Raw Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-2yqAarS8gr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#downlad data from GCS and store as pandas dataframe \n",
        "data_train = pd.read_csv(\n",
        "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing_train.csv',\n",
        "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"MEDV\"])\n",
        "\n",
        "data_test = pd.read_csv(\n",
        "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing_test.csv',\n",
        "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"MEDV\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRHrbAQaS8gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb9PFxjZS8gv",
        "colab_type": "text"
      },
      "source": [
        "#### Column Descriptions:\n",
        "\n",
        "1. CRIM: per capita crime rate by town \n",
        "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
        "3. INDUS: proportion of non-retail business acres per town \n",
        "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
        "5. NOX: nitric oxides concentration (parts per 10 million) \n",
        "6. RM: average number of rooms per dwelling \n",
        "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
        "8. DIS: weighted distances to five Boston employment centres \n",
        "9. RAD: index of accessibility to radial highways \n",
        "10. TAX: full-value property-tax rate per $10,000 \n",
        "11. PTRATIO: pupil-teacher ratio by town \n",
        "12. MEDV: Median value of owner-occupied homes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yN2J94WS8gw",
        "colab_type": "text"
      },
      "source": [
        "### 2) Write Tensorflow Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdPxQR8IS8gw",
        "colab_type": "text"
      },
      "source": [
        "#### 2.A Define Feature Columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5bWtidS8gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FEATURES = [\"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\",\n",
        "            \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\"]\n",
        "LABEL = \"MEDV\"\n",
        "\n",
        "feature_cols = [tf.feature_column.numeric_column(k)\n",
        "                  for k in FEATURES] #list of Feature Columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sGp74yCS8gz",
        "colab_type": "text"
      },
      "source": [
        "#### 2.B Define Estimator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61nTGW7NS8g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_estimator(output_dir):\n",
        "  return tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
        "                                            hidden_units=[10, 10],\n",
        "                                            model_dir=output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96OHOUQMS8g1",
        "colab_type": "text"
      },
      "source": [
        "#### 2.C Define Input Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqpqmSB7S8g4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_input_fn(data_set):\n",
        "    def input_fn():\n",
        "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
        "      labels = tf.constant(data_set[LABEL].values)\n",
        "      return features, labels\n",
        "    return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R-im1F5S8g7",
        "colab_type": "text"
      },
      "source": [
        "#### 2.D Define Serving Input Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ-UWzWfS8g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serving_input_fn():\n",
        "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
        "  feature_placeholders = {\n",
        "      column.name: tf.placeholder(column.dtype, [None])\n",
        "      for column in feature_cols\n",
        "  }\n",
        "  \n",
        "  #features are what we actually pass to the estimator\n",
        "  features = {\n",
        "    # Inputs are rank 1 so that we can provide scalars to the server\n",
        "    # but Estimator expects rank 2, so we expand dimension\n",
        "    key: tf.expand_dims(tensor, -1)\n",
        "    for key, tensor in feature_placeholders.items()\n",
        "  }\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "    features, feature_placeholders\n",
        "  )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkXH_pIUS8g9",
        "colab_type": "text"
      },
      "source": [
        "#### 2.E Define Train and Eval Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_X1p3IhS8g9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_spec = tf.estimator.TrainSpec(\n",
        "                input_fn=generate_input_fn(data_train),\n",
        "                max_steps=3000)\n",
        "\n",
        "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
        "\n",
        "eval_spec=tf.estimator.EvalSpec(\n",
        "            input_fn=generate_input_fn(data_test),\n",
        "            steps=1,\n",
        "            exporters=exporter)\n",
        "\n",
        "tf.estimator.train_and_evaluate(generate_estimator(output_dir), train_spec, eval_spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p--7lf5S8g_",
        "colab_type": "text"
      },
      "source": [
        "### 3) Package Code\n",
        "\n",
        "You've now written all the tensoflow code you need!\n",
        "\n",
        "**To make it compatible with Cloud ML Engine ≈we'll combine the above tensorflow code into a single python file with two simple changes**\n",
        "\n",
        "1. Add some boilerplate code to parse the command line arguments required for gcloud.\n",
        "2. Use the learn_runner.run() function to run the experiment\n",
        "\n",
        "We also add an empty \\__init__\\.py file to the folder. This is just the python convention for identifying modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps-tTZP_S8hA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "mkdir trainer\n",
        "touch trainer/__init__.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNJMSSnES8hE",
        "colab_type": "code",
        "outputId": "ce84964b-d1c2-41bc-d9f4-424bb12ca882",
        "colab": {}
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "data_train = pd.read_csv(\n",
        "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing_train.csv',\n",
        "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"MEDV\"])\n",
        "\n",
        "data_test = pd.read_csv(\n",
        "  filepath_or_buffer='https://storage.googleapis.com/vijay-public/boston_housing/housing_test.csv',\n",
        "  names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"MEDV\"])\n",
        "\n",
        "FEATURES = [\"CRIM\", \"ZN\", \"INDUS\", \"NOX\", \"RM\",\n",
        "            \"AGE\", \"DIS\", \"TAX\", \"PTRATIO\"]\n",
        "LABEL = \"MEDV\"\n",
        "\n",
        "feature_cols = [tf.feature_column.numeric_column(k)\n",
        "                  for k in FEATURES] #list of Feature Columns\n",
        "\n",
        "def generate_estimator(output_dir):\n",
        "  return tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
        "                                            hidden_units=[10, 10],\n",
        "                                            model_dir=output_dir)\n",
        "\n",
        "def generate_input_fn(data_set):\n",
        "    def input_fn():\n",
        "      features = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
        "      labels = tf.constant(data_set[LABEL].values)\n",
        "      return features, labels\n",
        "    return input_fn\n",
        "\n",
        "def serving_input_fn():\n",
        "  #feature_placeholders are what the caller of the predict() method will have to provide\n",
        "  feature_placeholders = {\n",
        "      column.name: tf.placeholder(column.dtype, [None])\n",
        "      for column in feature_cols\n",
        "  }\n",
        "  \n",
        "  #features are what we actually pass to the estimator\n",
        "  features = {\n",
        "    # Inputs are rank 1 so that we can provide scalars to the server\n",
        "    # but Estimator expects rank 2, so we expand dimension\n",
        "    key: tf.expand_dims(tensor, -1)\n",
        "    for key, tensor in feature_placeholders.items()\n",
        "  }\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "    features, feature_placeholders\n",
        "  )\n",
        "\n",
        "train_spec = tf.estimator.TrainSpec(\n",
        "                input_fn=generate_input_fn(data_train),\n",
        "                max_steps=3000)\n",
        "\n",
        "exporter = tf.estimator.LatestExporter('Servo', serving_input_fn)\n",
        "\n",
        "eval_spec=tf.estimator.EvalSpec(\n",
        "            input_fn=generate_input_fn(data_test),\n",
        "            steps=1,\n",
        "            exporters=exporter)\n",
        "\n",
        "######START CLOUD ML ENGINE BOILERPLATE######\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser()\n",
        "  # Input Arguments\n",
        "  parser.add_argument(\n",
        "      '--output_dir',\n",
        "      help='GCS location to write checkpoints and export models',\n",
        "      required=True\n",
        "  )\n",
        "  parser.add_argument(\n",
        "        '--job-dir',\n",
        "        help='this model ignores this field, but it is required by gcloud',\n",
        "        default='junk'\n",
        "    )\n",
        "  args = parser.parse_args()\n",
        "  arguments = args.__dict__\n",
        "  output_dir = arguments.pop('output_dir')\n",
        "######END CLOUD ML ENGINE BOILERPLATE######\n",
        "\n",
        "  #initiate training job\n",
        "  tf.estimator.train_and_evaluate(generate_estimator(output_dir), train_spec, eval_spec)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting trainer/task.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KADIoKOES8hI",
        "colab_type": "text"
      },
      "source": [
        "### 4) Train\n",
        "Now that our code is packaged we can invoke it using the gcloud command line tool to run the training. \n",
        "\n",
        "Note: Since our dataset is so small and our model is simple the overhead of provisioning the cluster is longer than the actual training time. Accordingly you'll notice the single VM cloud training takes longer than the local training, and the distributed cloud training takes longer than single VM cloud. For larger datasets and more complex models this will reverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQWu4iZ9S8hJ",
        "colab_type": "text"
      },
      "source": [
        "#### Set Environment Vars\n",
        "We'll create environment variables for our project name GCS Bucket and reference this in future commands.\n",
        "\n",
        "If you do not have a GCS bucket, you can create one using [these](https://cloud.google.com/storage/docs/creating-buckets) instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nMxzYI7S8hJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCS_BUCKET = 'gs://BUCKET_NAME' #CHANGE THIS TO YOUR BUCKET\n",
        "PROJECT = 'PROJECT_ID' #CHANGE THIS TO YOUR PROJECT ID\n",
        "REGION = 'us-central1' #OPTIONALLY CHANGE THIS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awDka6bJS8hL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['GCS_BUCKET'] = GCS_BUCKET\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['REGION'] = REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTAXEcqDS8hN",
        "colab_type": "text"
      },
      "source": [
        "#### Run local\n",
        "It's a best practice to first run locally on a small dataset to check for errors. Note you can ignore the warnings in this case, as long as there are no errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8A1A0wjS8hN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "gcloud ai-platform local train \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=trainer \\\n",
        "   -- \\\n",
        "   --output_dir='./output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4IZzG_jS8hP",
        "colab_type": "text"
      },
      "source": [
        "#### Run on cloud (1 cloud ML unit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVKoLbYwS8hQ",
        "colab_type": "text"
      },
      "source": [
        "First we specify which GCP project to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNJfi664S8hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQnxyKC2S8hV",
        "colab_type": "text"
      },
      "source": [
        "Then we specify which GCS bucket to write to and a job name.\n",
        "Job names submitted to the ml engine must be project unique, so we append the system date/time. Update the cell below to point to a GCS bucket you own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE2dgLWUS8hW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=./trainer \\\n",
        "   --job-dir=$GCS_BUCKET/$JOBNAME/ \\\n",
        "   --runtime-version 1.4 \\\n",
        "   -- \\\n",
        "   --output_dir=$GCS_BUCKET/$JOBNAME/output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXr5U2llS8hb",
        "colab_type": "text"
      },
      "source": [
        "#### Run on cloud (10 cloud ML units)\n",
        "Because we are using the TF Estimators interface, distributed computing just works! The only change we need to make to run in a distributed fashion is to add the [--scale-tier](https://cloud.google.com/ml/pricing#ml_training_units_by_scale_tier) argument. Cloud ML Engine then takes care of distributing the training across devices for you!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZoDHCqlS8hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=./trainer \\\n",
        "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
        "   --runtime-version 1.4 \\\n",
        "   --scale-tier=STANDARD_1 \\\n",
        "   -- \\\n",
        "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixlirH8HS8hd",
        "colab_type": "text"
      },
      "source": [
        "#### Run on cloud GPU (3 cloud ML units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCv5Wd1MS8hd",
        "colab_type": "text"
      },
      "source": [
        "Also works with GPUs!\n",
        "\n",
        "\"BASIC_GPU\" corresponds to one Tesla K80 at the time of this writing, hardware subject to change. 1 GPU is charged as 3 cloud ML units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmZc7C8DS8he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=./trainer \\\n",
        "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
        "   --runtime-version 1.4 \\\n",
        "   --scale-tier=BASIC_GPU \\\n",
        "   -- \\\n",
        "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osR0rCk-S8hi",
        "colab_type": "text"
      },
      "source": [
        "#### Run on 8 cloud GPUs (24 cloud ML units)\n",
        "To train across multiple GPUs you use a [custom scale tier](https://cloud.google.com/ml/docs/concepts/training-overview#job_configuration_parameters).\n",
        "\n",
        "You specify the number and types of machines you want to run on in a config.yaml, then reference that config.yaml via the --config config.yaml command line argument.\n",
        "\n",
        "Here I am specifying a master node with machine type complex_model_m_gpu and one worker node of the same type. Each complex_model_m_gpu has 4 GPUs so this job will run on 2x4=8 GPUs total. \n",
        "\n",
        "WARNING: The default project quota is 10 cloud ML units, so unless you have requested a quota increase you will get a quota exceeded error. This command is just for illustrative purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6d-ta9DS8hj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile config.yaml\n",
        "trainingInput:\n",
        "  scaleTier: CUSTOM\n",
        "  masterType: complex_model_m_gpu\n",
        "  workerType: complex_model_m_gpu\n",
        "  workerCount: 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2Eus2tS8hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "JOBNAME=housing_$(date -u +%y%m%d_%H%M%S)\n",
        "\n",
        "gcloud ai-platform jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=./trainer \\\n",
        "   --job-dir=$GCS_BUCKET/$JOBNAME \\\n",
        "   --runtime-version 1.4 \\\n",
        "   --config config.yaml \\\n",
        "   -- \\\n",
        "   --output_dir=$GCS_BUCKET/$JOBNAME/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-lZiaoSjer_w"
      },
      "source": [
        "### 5) Inspect Results Using Tensorboard\n",
        "\n",
        "Tensorboard is a utility that allows you to visualize your results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqbGBM-eS8ho",
        "colab_type": "text"
      },
      "source": [
        "### 6) Deploy Model For Predictions\n",
        "\n",
        "Cloud ML Engine has a prediction service that will wrap our tensorflow model with a REST API and allow remote clients to get predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q6oVuU7S8ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "MODEL_NAME=\"housing_prices\"\n",
        "MODEL_VERSION=\"v1\"\n",
        "MODEL_LOCATION=output/export/Servo/$(ls output/export/Servo | tail -1) \n",
        "\n",
        "#gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME} #Uncomment to overwrite existing version\n",
        "#gcloud ai-platform models delete ${MODEL_NAME} #Uncomment to overwrite existing model\n",
        "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
        "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --staging-bucket=$GCS_BUCKET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ialLCsbwS8hq",
        "colab_type": "text"
      },
      "source": [
        "### 7) Get Predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcKAzGgPS8hr",
        "colab_type": "code",
        "outputId": "c5a4f0cb-3c0a-4039-b17a-e81037d0830d",
        "colab": {}
      },
      "source": [
        "%%writefile records.json\n",
        "{\"CRIM\": 0.00632,\"ZN\": 18.0,\"INDUS\": 2.31,\"NOX\": 0.538, \"RM\": 6.575, \"AGE\": 65.2, \"DIS\": 4.0900, \"TAX\": 296.0, \"PTRATIO\": 15.3}\n",
        "{\"CRIM\": 0.00332,\"ZN\": 0.0,\"INDUS\": 2.31,\"NOX\": 0.437, \"RM\": 7.7, \"AGE\": 40.0, \"DIS\": 5.0900, \"TAX\": 250.0, \"PTRATIO\": 17.3}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing records.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lVDcikGxgVKM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s9P70y5S8hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gcloud ai-platform predict --model housing_prices --json-instances records.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9L_qTtXS8hw",
        "colab_type": "text"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "#### What we covered\n",
        "1. How to use Tensorflow's high level Estimator API\n",
        "2. How to deploy tensorflow code for distributed training in the cloud\n",
        "3. How to evaluate results using TensorBoard\n",
        "4. How deploy the resulting model to the cloud for online prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSQB6LzUibZJ",
        "colab_type": "text"
      },
      "source": [
        "[The source](https://github.com/vijaykyr/tensorflow_teaching_examples/blob/master/housing_prices/cloud-ml-housing-prices.ipynb)"
      ]
    }
  ]
}